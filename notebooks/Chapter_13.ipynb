{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cling import cling, bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 13.1**\n",
    "\n",
    "Caption: Implementation of the fractal set using the Message Passing Interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi_fractal.cpp\n",
    "#include <mpi.h>\n",
    "#include <chrono>\n",
    "\n",
    "#include <config.hpp>\n",
    "#include <pbm.hpp>\n",
    "#include <kernel.hpp>\n",
    "#include <util.hpp>\n",
    "#include <io.hpp>\n",
    "//-----------------------------------------------------------------\n",
    "\n",
    "using std::chrono::high_resolution_clock;\n",
    "int main() {\n",
    "  // Initialize the MPI environment\n",
    "  MPI_Init(NULL, NULL); \n",
    "  // Get the number of processes or nodes\n",
    "  int mpi_size;\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size); \n",
    "  // Get the rank of the process\n",
    "  int mpi_rank;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank); \n",
    "  // Start the timer\n",
    "  high_resolution_clock::time_point start_time;\n",
    "  PBM pbm;\n",
    "  if (mpi_rank == 0){\n",
    "    start_time  = high_resolution_clock::now(); \n",
    "  }\n",
    "  pbm = PBM(size_y, size_x);\n",
    "\n",
    "  std::size_t iter_start, iter_end;\n",
    "  split_work(mpi_size, size_x, mpi_rank, iter_start, iter_end); \n",
    "\n",
    "  for (size_t iter = iter_start; iter < iter_end; iter++) { \n",
    "    pbm.row(iter) = compute_row(iter);\n",
    "    assert(pbm.row(iter).size() == size_y);\n",
    "    if(mpi_rank > 0) {\n",
    "       MPI_Request req;\n",
    "       MPI_Isend((void*)pbm.row(iter).data(),size_y,MPI_INT,0,iter,MPI_COMM_WORLD,&req); \n",
    "    }\n",
    "  } \n",
    "\n",
    "  // Save the image\n",
    "  if (mpi_rank == 0) { \n",
    "     save_image(pbm, mpi_size, size_x, size_y);\n",
    "  }\n",
    "  // Stop the timer\n",
    "  if (mpi_rank == 0){\n",
    "    auto stop_time = std::chrono::high_resolution_clock::now();\n",
    "    auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(\n",
    "        stop_time - start_time);\n",
    "    std::cout << \"Time: \" << duration.count()*1e-9 << std::endl;\n",
    "  }\n",
    "  // Finalize the MPI environment.\n",
    "  MPI_Finalize();  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mpicxx -I . -o mpi_fractal.exe mpi_fractal.cpp\n",
    "mpirun --bind-to none -np 3 ./mpi_fractal.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 13.2**\n",
    "\n",
    "Caption: Utility functions to save the PBM image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile io.hpp\n",
    "#ifndef IO_HPP\n",
    "#define IO_HPP\n",
    "\n",
    "void save_image(PBM &pbm, size_t mpi_size, size_t size_x,\n",
    "                size_t size_y) {\n",
    "  std :: size_t iter_start , iter_end ;\n",
    "  for(int worker=1;worker<mpi_size;worker++) { \n",
    "    split_work(mpi_size, size_x, worker, iter_start, iter_end);\n",
    "    for(size_t iter=iter_start;iter < iter_end; ++iter) {  \n",
    "      std::vector<int> v; \n",
    "      v.resize(size_y); \n",
    "      MPI_Status status;\n",
    "      MPI_Recv((void*)v.data(),size_y,MPI_INT,worker,iter,MPI_COMM_WORLD,&status);   \n",
    "      assert(pbm.row(iter).size() == v.size());\n",
    "      pbm.row(iter) = v; \n",
    "    }\n",
    "  }\n",
    "  // Save the image\n",
    "  pbm.save(\"image_mpi.pbm\"); \n",
    "}\n",
    "#endif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 13.3**\n",
    "\n",
    "Caption: Implementation of the fractal set using the Message Passing Interface with distributed IO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi_fractal_improved.cpp\n",
    "#include <mpi.h>\n",
    "#include <chrono>\n",
    "#include <fstream> \n",
    "\n",
    "#include <config.hpp>\n",
    "#include <kernel.hpp>\n",
    "#include <util.hpp>\n",
    "\n",
    "using std::chrono::high_resolution_clock;\n",
    "//-----------------------------------------------------------------\n",
    "int main() {\n",
    "  // Initialize the MPI environment\n",
    "  MPI_Init(NULL, NULL);\n",
    "  // Get the number of processes or nodes\n",
    "  int mpi_size;\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n",
    "  // Get the rank of the process\n",
    "  int mpi_rank;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n",
    "  // Start the timer\n",
    "  high_resolution_clock::time_point start_time;\n",
    "  std::ofstream outfile(\"data_\"+\n",
    "    std::to_string(mpi_rank)+\".part\"); \n",
    "  if (mpi_rank == 0)\n",
    "    start_time  = high_resolution_clock::now();\n",
    "\n",
    "  std::size_t iter_start, iter_end;\n",
    "  split_work(mpi_size, size_x, mpi_rank, iter_start, iter_end);\n",
    "\n",
    "  for (size_t iter = iter_start; iter < iter_end; iter++) {\n",
    "    std::vector<int> row = compute_row(iter);\n",
    "\n",
    "    assert(row.size() == size_y);\n",
    "    outfile << iter << \" \";\n",
    "    for(auto color : row) \n",
    "      outfile << color << \" \";\n",
    "      outfile << \"\\n\";\n",
    "  }\n",
    "\n",
    "  // Stop the timer\n",
    "  if (mpi_rank == 0){\n",
    "    auto stop_time = std::chrono::high_resolution_clock::now();\n",
    "    auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(\n",
    "        stop_time - start_time);\n",
    "    std::cout << \"Time: \" << duration.count()*1e-9 << std::endl;\n",
    "  }\n",
    "  outfile.close(); \n",
    "  // Finalize the MPI environment.\n",
    "  MPI_Finalize();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mpicxx -I . -o mpi_fractal_improved.exe mpi_fractal_improved.cpp\n",
    "mpirun --bind-to none -np 3 ./mpi_fractal_improved.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 13.4**\n",
    "\n",
    "Caption: Code to combine the distributed image data into a single PBM image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile combine_image.cpp\n",
    "#include <fstream> \n",
    "#include <iostream>\n",
    "#include <pbm.hpp> \n",
    "//-----------------------------------------------------------------\n",
    "std::tuple<int, std::vector<int>> split(std::string line) { \n",
    "\n",
    "  std::istringstream stream(line); \n",
    "  std::string index;\n",
    "  std::vector<int> row;\n",
    "\n",
    "  std::string s;\n",
    "  bool first = true;\n",
    "  while (getline(stream, s, ' ')) { \n",
    "    if (first) {\n",
    "      index = s;\n",
    "      first = false;\n",
    "    } else\n",
    "      row.push_back(std::stoi(s));\n",
    "  }\n",
    "  return std::make_tuple(std::stoi(index), row);\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "\n",
    "  PBM pbm = PBM(size_y, size_x); \n",
    "\n",
    "  size_t id = 0;\n",
    "\n",
    "  while (true) {  \n",
    "\n",
    "    std::ifstream file; \n",
    "    file.open(\"data_\" + std::to_string(id) + \".part\");\n",
    "    if(!file.good()) {\n",
    "        std::cout << \"Files read: \" << id << std::endl;\n",
    "        break;\n",
    "    }\n",
    "\n",
    "    int index; \n",
    "    std::vector<int> row; \n",
    "\n",
    "    std::string s;\n",
    "    while (std::getline(file, s)) { \n",
    "\n",
    "      auto res = split(s);  \n",
    "\n",
    "      index = std::get<0>(res); \n",
    "      row = std::get<1>(res); \n",
    "      pbm.row(index) = row; \n",
    "    }\n",
    "    id++;\n",
    "  }\n",
    "\n",
    "  pbm.save(\"image_combined.pbm\"); \n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "g++ -std=c++20 -I . -o combine_image.exe combine_image.cpp -lpthread\n",
    "./combine_image.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 13.5**\n",
    "\n",
    "Caption: Implementation of the fractal set using MPI + OpenMP: Using a critical section for the distributed I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cling\n",
    "#pragma omp parallel for \n",
    "for (size_t iter = iter_start; iter < iter_end; iter++) {\n",
    "  std::vector<int> row = compute_row(iter);\n",
    "\n",
    "  assert(row.size() == size_y);\n",
    "  #pragma omp critical \n",
    "  {\n",
    "    outfile << iter << \" \";\n",
    "    for(auto color : row) {\n",
    "      outfile << color << \" \";\n",
    "    }\n",
    "    outfile << \"\\n\";\n",
    "  } \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 13.6**\n",
    "\n",
    "Caption: Improved implementation of fractal set using MPI+OpenMP without using a scope lock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi_openmp_fractal.cpp\n",
    "#include <mpi.h>\n",
    "#include <chrono>\n",
    "#include <fstream>\n",
    "#include <config.hpp>\n",
    "#include <kernel.hpp>\n",
    "#include <util.hpp>\n",
    "\n",
    "using std::chrono::high_resolution_clock;\n",
    "//-----------------------------------------------------------------\n",
    "int main() {\n",
    "  // Initialize the MPI environment\n",
    "  MPI_Init(NULL, NULL);\n",
    "  // Get the number of processes or nodes\n",
    "  int mpi_size;\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n",
    "  // Get the rank of the process\n",
    "  int mpi_rank;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n",
    "  // Start the timer\n",
    "  high_resolution_clock::time_point start_time;\n",
    "  std::ofstream outfile (\"data_\"+std::to_string(mpi_rank)+\".part\");\n",
    "  if (mpi_rank == 0)\n",
    "    start_time  = high_resolution_clock::now();\n",
    "\n",
    "  std::size_t iter_start, iter_end;\n",
    "  split_work(mpi_size, size_x, mpi_rank, iter_start, iter_end); \n",
    "\n",
    "  std::vector<std::vector<int>> data(iter_end-iter_start); \n",
    "\n",
    "  #pragma omp parallel for   \n",
    "  for (size_t iter = iter_start; iter < iter_end; iter++) { \n",
    "    data[iter-iter_start] = compute_row(iter);\n",
    "  }\n",
    "\n",
    "  size_t iter  = iter_start;  \n",
    "  for( auto row : data){\n",
    "     outfile << iter << \" \";\n",
    "     for(auto color : row)\n",
    "       outfile << color << \" \";\n",
    "     outfile << \"\\n\";\n",
    "     iter++;\n",
    "  }  \n",
    "\n",
    "  // Stop the timer\n",
    "  if (mpi_rank == 0){\n",
    "    auto stop_time = std::chrono::high_resolution_clock::now();\n",
    "    auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(\n",
    "    stop_time - start_time);\n",
    "    std::cout << \"Time: \" << duration.count()*1e-9 << std::endl;\n",
    "  }\n",
    "  outfile.close();\n",
    "  // Finalize the MPI environment.\n",
    "  MPI_Finalize();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mpicxx -I . -o mpi_openmp_fractal.exe mpi_openmp_fractal.cpp\n",
    "mpirun --bind-to none -np 3 ./mpi_openmp_fractal.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 13.7**\n",
    "\n",
    "Caption: Sample bash script to to run the distributed MPI-based fractal set code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "# Figure out the number of cores\n",
    "export CORES=$(lscpu|grep \"^CPU(s):\"|cut -d: -f2|sed 's/\\s//g')\n",
    "echo \"CORES=${CORES}\"\n",
    "# Let OpenMP use half the machine\n",
    "export OMP_NUM_THREADS=$((${CORES}/2))\n",
    "echo \"OMP_NUM_THREADS=${OMP_NUM_THREADS}\" \n",
    "export SIZE_X=$((${CORES}*100)) \n",
    "export SIZE_Y=$((${CORES}*100))\n",
    "echo \"SIZE_X=${SIZE_X} SIZE_Y=${SIZE_Y}\"\n",
    "export TYPE=julia \n",
    "# Run in two processes\n",
    "mpirun --bind-to none -N 2 ./mpi_fractal.exe \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
